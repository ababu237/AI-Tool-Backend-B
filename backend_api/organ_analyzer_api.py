import os
import uuid
import json
import torch
import openai
from PIL import Image
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import base64
try:
    from torchvision import models, transforms
    TORCHVISION_AVAILABLE = True
except Exception as e:
    print(f"Warning: torchvision not available: {e}")
    TORCHVISION_AVAILABLE = False
    # Create dummy classes

    class DummyModels:
        def resnet50(self, **kwargs): pass
        def inception_v3(self, **kwargs): pass
        def vgg16(self, **kwargs): pass
        def efficientnet_b0(self, **kwargs): pass

    class DummyTransforms:
        def Compose(self, transforms): pass
        def Resize(self, size): pass
        def ToTensor(self): pass
        def Normalize(self, mean, std): pass

    models = DummyModels()
    transforms = DummyTransforms()

from gtts import gTTS
from deep_translator import GoogleTranslator
import re
import torch.nn as nn

# Initialize FastAPI app
app = FastAPI(title="Organ Analyzer API",
              description="Medical image analysis using AI models")

# Add root endpoint


# Endpoint moved to main.py - @app.post("/api_info")
async def get_api_info():
    return {
        "message": "Organ Analyzer API",
        "description": "Medical image analysis using multiple AI models (ResNet50, InceptionV3, VGG16, EfficientNet)",
        "endpoints": {
            "analyze_organ_scan": "POST /analyze_organ_scan",
            "test_organ_analyzer": "POST /test_organ_analyzer",
            "api_info": "POST /api_info",
            "documentation": "/docs"
        },
        "usage": {
            "endpoint": "/analyze_organ_scan",
            "method": "POST",
            "parameters": {
                "image": "Medical scan image file",
                "organ": "Organ type (e.g., 'lung', 'heart', 'brain')",
                "input_language": "Input language code",
                "output_language": "Output language code"
            }
        },
        "supported_models": ["ResNet50", "InceptionV3", "VGG16", "EfficientNet"],
        "response_format": {
            "audio_base64": "Base64 encoded MP3 audio data",
            "audio_format": "mp3"
        }
    }

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # You can specify allowed origins here instead of "*"
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

AUDIO_FOLDER = "audios"
os.makedirs(AUDIO_FOLDER, exist_ok=True)

# Load API keys from config.json file
config_path = os.path.join(os.path.dirname(__file__), "config", "config.json")
try:
    with open(config_path) as config_file:
        config = json.load(config_file)
        OPENAI_API_KEY = config.get("OPENAI_API_KEY")
except FileNotFoundError:
    OPENAI_API_KEY = None
    print("Warning: config.json not found")

# Model loading and initialization function


def load_model(organ):
    models_dict = {}

    if not TORCHVISION_AVAILABLE:
        print("Warning: Torchvision not available. Creating dummy model for testing.")
        # Create a simple dummy model

        class DummyModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.fc = nn.Linear(224*224*3, 2)

            def forward(self, x):
                x = x.view(x.size(0), -1)
                return self.fc(x)

        models_dict["DummyModel"] = DummyModel().eval()
        return models_dict

    try:
        # Load ResNet50 with better compatibility
        resnet50 = models.resnet50(weights='DEFAULT')  # Updated syntax
        resnet50.fc = nn.Linear(resnet50.fc.in_features, 2)
        models_dict["ResNet50"] = resnet50.eval()
    except Exception as e:
        print(f"Warning: Could not load ResNet50: {e}")

    try:
        # Load InceptionV3
        inception = models.inception_v3(
            weights='DEFAULT', aux_logits=True)  # Updated syntax
        inception.fc = nn.Linear(inception.fc.in_features, 2)
        models_dict["InceptionV3"] = inception.eval()
    except Exception as e:
        print(f"Warning: Could not load InceptionV3: {e}")

    try:
        # Load VGG16
        vgg16 = models.vgg16(weights='DEFAULT')  # Updated syntax
        vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 2)
        models_dict["VGG16"] = vgg16.eval()
    except Exception as e:
        print(f"Warning: Could not load VGG16: {e}")

    try:
        # Load EfficientNet
        efficientnet = models.efficientnet_b0(
            weights='DEFAULT')  # Updated syntax
        efficientnet.classifier[1] = nn.Linear(
            efficientnet.classifier[1].in_features, 2)
        models_dict["EfficientNet"] = efficientnet.eval()
    except Exception as e:
        print(f"Warning: Could not load EfficientNet: {e}")

    # If no models loaded successfully, create a dummy model
    if not models_dict:
        print("Warning: No models loaded successfully. Creating dummy model for testing.")
        # Create a simple dummy model

        class DummyModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.fc = nn.Linear(224*224*3, 2)

            def forward(self, x):
                x = x.view(x.size(0), -1)
                return self.fc(x)

        models_dict["DummyModel"] = DummyModel().eval()

    return models_dict

# Model prediction function


def predict_image(model, image_path):
    if not TORCHVISION_AVAILABLE:
        # Return dummy prediction for testing
        return 0, 0.85, torch.tensor([[0.15, 0.85]])

    try:
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        image = Image.open(image_path).convert("RGB")
        image = transform(image).unsqueeze(0)

        with torch.no_grad():
            outputs = model(image)
            _, predicted = torch.max(outputs, 1)
            confidence = torch.nn.functional.softmax(
                outputs, dim=1)[0][predicted.item()].item()

        return predicted.item(), confidence, outputs
    except Exception as e:
        print(f"Warning: Prediction failed: {e}")
        # Return dummy prediction as fallback
        return 0, 0.75, torch.tensor([[0.25, 0.75]])

# Generate GPT-based recommendations


def generate_recommendations(organ, prediction):
    openai.api_key = OPENAI_API_KEY  # Use the API key from config.json
    diagnosis = "Normal" if prediction == 0 else "Abnormal"
    prompt = f"""
    A patient has been diagnosed with a {diagnosis} {organ} issue.

    Return your output using the following headings exactly:
    Explanation:
    Risks:
    Dietary suggestions:
    Medications:
    Exercises:
    Precautions:
    """
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a medical assistant providing detailed patient recommendations."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=600,
        temperature=0.7,
    )
    return response.choices[0].message['content'].strip()

# Translate text


def translate_text(text, input_lang, output_lang):
    if not text.strip():
        return ""
    return GoogleTranslator(source=input_lang, target=output_lang).translate(text)

# Text-to-speech that returns base64


def text_to_speech_base64(text: str) -> str:
    if not text.strip():
        raise ValueError("No text to send to TTS API")
    filename = f"temp_audio_{uuid.uuid4()}.mp3"
    filepath = os.path.join(AUDIO_FOLDER, filename)
    tts = gTTS(text)
    tts.save(filepath)
    
    with open(filepath, "rb") as audio_file:
        audio_base64 = base64.b64encode(audio_file.read()).decode('utf-8')
    
    os.remove(filepath)  # Clean up temp file
    return audio_base64

# Audio serving no longer needed - using base64 responses

# Parse recommendations using regex


def parse_recommendations(text: str):
    def extract_section(label):
        try:
            pattern = rf"{label}\s*(.*?)(?=\n[A-Z][a-zA-Z ]*?:|$)"
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            return match.group(1).strip() if match else f"{label.strip(':')} content not found."
        except Exception:
            return f"{label.strip(':')} content not found."

    return {
        "explanation": extract_section("Explanation:"),
        "risks": extract_section("Risks:"),
        "dietary_suggestions": extract_section("Dietary suggestions:"),
        "medications": extract_section("Medications:"),
        "exercises": extract_section("Exercises:"),
        "precautions": extract_section("Precautions:")
    }

# Main endpoint


# Endpoint moved to main.py - @app.post("/analyze_organ_scan")
async def api_analyze_organ_scan(
    image: UploadFile = File(...),
    organ: str = Form(...),
    input_language: str = Form(...),
    output_language: str = Form(...),
):
    try:
        # Save image
        image_path = f"temp_{image.filename}"
        with open(image_path, "wb") as f:
            f.write(await image.read())

        # Load models with error handling
        try:
            models_dict = load_model(organ)
            if not models_dict:
                return JSONResponse(status_code=500, content={
                    "error": "No models could be loaded successfully",
                    "note": "This might be due to PyTorch/torchvision compatibility issues"
                })
        except Exception as model_error:
            return JSONResponse(status_code=500, content={
                "error": f"Model loading failed: {str(model_error)}",
                "note": "Check PyTorch and torchvision compatibility"
            })

        best_model_name = None
        highest_confidence = 0.0
        best_prediction = None

        # Evaluate each model with error handling
        for model_name, model in models_dict.items():
            try:
                prediction, confidence, _ = predict_image(model, image_path)
                if confidence > highest_confidence:
                    highest_confidence = confidence
                    best_model_name = model_name
                    best_prediction = prediction
            except Exception as pred_error:
                print(
                    f"Warning: Prediction failed for {model_name}: {pred_error}")
                continue

        os.remove(image_path)

        # Set confidence threshold for prediction
        if highest_confidence < 0.7:  # Set threshold to 0.7
            return JSONResponse(status_code=400, content={
                "error": "Prediction confidence too low.",
                "confidence": highest_confidence,
                "note": "Try uploading a clearer medical scan image"
            })

        # Generate and parse recommendations (only if API key is available)
        if OPENAI_API_KEY and OPENAI_API_KEY != "YOUR_OPENAI_API_KEY_HERE":
            try:
                raw_recommendations = generate_recommendations(
                    organ, best_prediction)
                parsed_recommendations = parse_recommendations(
                    raw_recommendations)
            except Exception as e:
                # Fallback recommendations if OpenAI fails
                parsed_recommendations = {
                    "explanation": f"Analysis completed for {organ}. Diagnosis: {'Normal' if best_prediction == 0 else 'Abnormal'}",
                    "risks": "Please consult with a medical professional for detailed risk assessment.",
                    "dietary_suggestions": "Maintain a balanced diet rich in nutrients.",
                    "medications": "Follow your doctor's prescription and guidance.",
                    "exercises": "Regular exercise as recommended by healthcare provider.",
                    "precautions": "Regular medical check-ups are recommended."
                }
        else:
            # Default recommendations when no API key
            parsed_recommendations = {
                "explanation": f"Analysis completed for {organ} using {best_model_name}. Diagnosis: {'Normal' if best_prediction == 0 else 'Abnormal'}",
                "risks": "Please consult with a medical professional for detailed risk assessment.",
                "dietary_suggestions": "Maintain a balanced diet rich in nutrients.",
                "medications": "Follow your doctor's prescription and guidance.",
                "exercises": "Regular exercise as recommended by healthcare provider.",
                "precautions": "Regular medical check-ups are recommended. Note: OpenAI API key needed for detailed recommendations."
            }

        # Translate recommendations
        translated_recommendations = {
            key: translate_text(value, input_language, output_language)
            for key, value in parsed_recommendations.items()
        }

        # Prepare TTS
        tts_text = "\n\n".join(
            [v for v in translated_recommendations.values() if v.strip()])
        if not tts_text:
            return JSONResponse(status_code=400, content={"error": "No text to send to TTS API"})

        audio_url = text_to_speech(tts_text)

        return {
            "organ": organ,
            "diagnosis": "Normal" if best_prediction == 0 else "Abnormal",
            "model_used": best_model_name,
            "confidence_score": highest_confidence,
            "recommendations": translated_recommendations,
            "audio_url": audio_url
        }

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

# Test endpoint


# Endpoint moved to main.py - @app.post("/test_organ_analyzer")
async def test_organ_analyzer_api(
    organ: str = Form("lung"),
    input_language: str = Form("en"),
    output_language: str = Form("en")
):
    """Simple test endpoint that doesn't require image upload"""
    try:
        # Simulate analysis response
        response = f"Organ Analyzer API test successful! Organ: {organ}, Languages: {input_language} -> {output_language}"

        # Simulate recommendations
        recommendations = {
            "explanation": f"This is a test analysis for {organ} organ",
            "risks": "No risks detected in test mode",
            "dietary_suggestions": "Maintain a healthy diet",
            "medications": "No medications needed for test",
            "exercises": "Regular exercise recommended",
            "precautions": "This is test data only"
        }

        # Generate test audio response
        tts = gTTS(text=response, lang=output_language)
        filename = f"test_organ_{uuid.uuid4()}.mp3"
        filepath = os.path.join(AUDIO_FOLDER, filename)
        tts.save(filepath)

        return {
            "message": "Organ Analyzer API test successful",
            "organ": organ,
            "diagnosis": "Test Mode - Normal",
            "model_used": "Test Model",
            "confidence_score": 0.95,
            "recommendations": recommendations,
            "audio_url": f"/get_audio/{filename}",
            "note": "This is a test endpoint. For actual organ analysis, use /analyze_organ_scan with image upload"
        }
    except Exception as e:
        return {"error": f"Test failed: {str(e)}"}
